# To play the game "LunarLander-v2" using PyTorch, you'll need to integrate the game environment with a PyTorch model. You can train a neural network using reinforcement learning (e.g., Deep Q-Networks, or DQN) and then use it to interact with the environment. Hereâ€™s a basic outline of how to set it up:

# ### 1. **Install required libraries:**
# Make sure you have the necessary libraries installed:
# ```bash
#pip install gymnasium torch numpy
# ```

# ### 2. **Define the DQN architecture:**
# Define a simple neural network for the agent using PyTorch. This network will learn to take actions based on the state from the game.

# ```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np

# Define a simple fully connected neural network
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)
# ```

# ### 3. **Initialize the environment and model:**

# ```python
import gymnasium as gym
import torch

env = gym.make("LunarLander-v2", render_mode="human")
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

# Create the DQN model
model = DQN(input_dim=state_dim, output_dim=action_dim)
# ```

# ### 4. **Define the training loop:**
# In this section, we'll define how the agent interacts with the environment, how rewards are collected, and how the model is updated.

# ```python
# Parameters
learning_rate = 0.001
gamma = 0.99  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_decay = 0.995
epsilon_min = 0.01
episodes = 500

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Function to choose action (using epsilon-greedy policy)
def choose_action(state, epsilon):
    if np.random.rand() <= epsilon:
        return np.random.choice(action_dim)  # Random action
    state = torch.FloatTensor(state).unsqueeze(0)
    with torch.no_grad():
        q_values = model(state)
    return torch.argmax(q_values).item()

# Function to train the model
def train_model(memory, batch_size=64):
    if len(memory) < batch_size:
        return
    batch = np.random.choice(memory, batch_size)

    for state, action, reward, next_state, done in batch:
        state = torch.FloatTensor(state).unsqueeze(0)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        reward = torch.FloatTensor([reward])
        done = torch.FloatTensor([done])

        q_value = model(state)[0, action]
        next_q_value = torch.max(model(next_state)).item()

        q_target = reward + (1 - done) * gamma * next_q_value

        loss = F.mse_loss(q_value, q_target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# Main loop
memory = []

for episode in range(episodes):
    state = env.reset()[0]
    total_reward = 0

    for t in range(1000):
        action = choose_action(state, epsilon)
        next_state, reward, done, truncated, _ = env.step(action)
        memory.append((state, action, reward, next_state, done))

        train_model(memory)

        state = next_state
        total_reward += reward
        if done or truncated:
            break

    epsilon = max(epsilon_min, epsilon * epsilon_decay)
    print(f"Episode {episode + 1}, Total Reward: {total_reward}")

env.close()
# ```

# ### 5. **Explanation of Key Components:**
# - **Epsilon-Greedy Policy:** This balances exploration and exploitation. Initially, the agent will explore randomly, but over time it will favor actions suggested by the DQN model.
# - **Training the Model:** The `train_model` function samples random batches from the memory and updates the model using the mean squared error loss between predicted and target Q-values.
# - **Memory Buffer:** Stores the experience (state, action, reward, next state, done) to train the model.

# This is a basic structure to get you started. You can improve this by adding more advanced techniques like experience replay buffers, target networks, or using other reinforcement learning algorithms such as Proximal Policy Optimization (PPO) or Advantage Actor-Critic (A2C).
